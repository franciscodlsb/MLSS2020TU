{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLSS_practical_session_object_detection_tf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franciscodlsb/MLSS2020TU/blob/master/MLSS_practical_session_object_detection_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT-Sz5w-x9A5",
        "colab_type": "text"
      },
      "source": [
        "# MLSS-Indonesia Practical Session: Object Detection Using Google Colab\n",
        "\n",
        "In this session we'll use Google colab to demonstrate how a computer vision technique (using deep learning) detect objects in images. You'll learn\n",
        "*   Step by step process of using a pre-trained model to detect objects in an image (out-of-the-box detection).\n",
        "*   Finetuning a (TF2 friendly) RetinaNet architecture on (very) few examples of a novel class after initializing the weights from a pre-trained COCO checkpoint. Training runs in eager mode.\n",
        "\n",
        "Estimated time to run through this colab (with GPU): < 45 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-9kQcwVpJlf",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification vs Object Detection\n",
        "\n",
        "Image classification: \n",
        "*   Try to answer: WHAT *is* in the image?\n",
        "*   Limitation: Object is in the majority of the image. Works well if the image contains only single object.\n",
        "\n",
        "Object detection: \n",
        "*   Try to answer: WHAT *are* in the image **and** WHERE they are\n",
        "*   Advantage: Can detect multiple objects at a time, and can handle small objects. \n",
        "\n",
        "\n",
        "![classification vs object detection](https://i.ibb.co/jZRJsrt/object-detection.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNXre8h7XOXL",
        "colab_type": "text"
      },
      "source": [
        "Typically, there are three steps in an object detection framework.\n",
        "\n",
        "1. Object localisation component, a model or algorithm is used to generate regions of interest (or region proposal). In practice, these are a large set of bounding boxes which could span the full image. Example: Sliding window (inefficient), selective search (clustering algos), Region Proposal Network (deep learning)\n",
        "2. Object classification component, visual features are extracted for each of the bounding boxes, evaluated, and determined which objects are present in the region based on visual features. Example: Kernel based, histogram method, deep learning.\n",
        "3. In the final post-processing step, overlapping boxes are combined into a single bounding box (this is called non maximum suppression).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfD2mxwzaBsw",
        "colab_type": "text"
      },
      "source": [
        "## Region-based CNN\n",
        "![rcnn](https://i.ibb.co/KLrzrxG/rcnn.png)*R-CNN*\n",
        "![fast-rcnn](https://i.ibb.co/mXMWSfZ/fast-rcnn.jpg)*Fast-RCNN*\n",
        "![faster-rcnn](https://i.ibb.co/YQYqjX4/faster-rcnn.png)*Faster-RCNN*\n",
        "\n",
        "## SSD\n",
        "![ssd](https://i.ibb.co/4Smq9jb/ssd.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqAiZ3rq09_u",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCgPQHXgC44j",
        "colab_type": "text"
      },
      "source": [
        "Checking if Colab is running on GPU \n",
        "\n",
        "*   you should see ‘Found GPU’ and tf version\n",
        "*   If you see an Error change the runtime to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN0eUGe8C5ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "   raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print('TensorFlow version: {}'.format(tf.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbH5d_Lhv7v",
        "colab_type": "text"
      },
      "source": [
        "#TensorFlow Object Detection API (ODAPI) library. \n",
        "The Tensorflow ODAPI brings together above ideas (and more) together in a single package, allowing us to quickly iterate over different configurations using the Tensorflow backend. With the API, we are defining the object detection model using configuration files, and the Tensorflow ODAPI is responsible for structuring all the necessary elements together.\n",
        "\n",
        "Note: the following libraries are neccessary for the package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQBUJSQIJD_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf_slim\n",
        "!pip install pycocotools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phkM4NhC1w8m",
        "colab_type": "text"
      },
      "source": [
        "Cloning `tensorflow/models` repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zORY0G4uJE0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNs4iPTm2AdH",
        "colab_type": "text"
      },
      "source": [
        "Cloning files needed for testing & finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6qxUTkoJLtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --depth 1 https://github.com/ronygustam/MLSS-INDONESIA-2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKJnWi0f2N04",
        "colab_type": "text"
      },
      "source": [
        "Compiling using protobufs and installing the object_detection package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD5xFk99JSuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etavdH1O2aHV",
        "colab_type": "text"
      },
      "source": [
        "# Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nai2NKIJfRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob, imageio\n",
        "import io, random, scipy.misc\n",
        "import os, sys, tarfile, zipfile\n",
        "import six.moves.urllib as urllib\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from io import StringIO\n",
        "from six import BytesIO\n",
        "from collections import defaultdict\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aSBdS1k2v6o",
        "colab_type": "text"
      },
      "source": [
        "Import the object detection module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpbSX-M_JjPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from object_detection.utils import colab_utils\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "from object_detection.builders import model_builder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TYBSDfV20FM",
        "colab_type": "text"
      },
      "source": [
        "Patching some of TensorFlow utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TuPBX8EJlYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# patch tf1 into `utils.ops`\n",
        "utils_ops.tf = tf.compat.v1\n",
        "\n",
        "# Patch the location of gfile\n",
        "tf.gfile = tf.io.gfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH4Bxp0F3LLM",
        "colab_type": "text"
      },
      "source": [
        "# Model preparation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHqcFpdO3cBv",
        "colab_type": "text"
      },
      "source": [
        "## Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So04Vb9MJn37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(model_name):\n",
        "  base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
        "  model_file = model_name + '.tar.gz'\n",
        "  model_dir = tf.keras.utils.get_file(\n",
        "    fname=model_name, \n",
        "    origin=base_url + model_file,\n",
        "    untar=True)\n",
        "\n",
        "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
        "  model = tf.saved_model.load(str(model_dir))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dglNJ6hO3NpF",
        "colab_type": "text"
      },
      "source": [
        "## Variables\n",
        "\n",
        "We'll use \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.\n",
        "\n",
        "For future use, any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing the path."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GjD6C1V3dLF",
        "colab_type": "text"
      },
      "source": [
        "## Loading label map\n",
        "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAtRq6NEJqDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr-YKG7i3umK",
        "colab_type": "text"
      },
      "source": [
        "For the sake of simplicity we will test on 2 images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU9iadP-3y6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The library has some test images located in the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('models/research/object_detection/test_images')\n",
        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
        "TEST_IMAGE_PATHS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55otVSVjJsqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('MLSS-INDONESIA-2020/dataset/fried-chicken/test')\n",
        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
        "TEST_IMAGE_PATHS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeeEqY_Y37t5",
        "colab_type": "text"
      },
      "source": [
        "# Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7qGc7IH3-eU",
        "colab_type": "text"
      },
      "source": [
        "Load an object detection model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ-8CQbbJ74x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
        "detection_model = load_model(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjvJiSyc4LCg",
        "colab_type": "text"
      },
      "source": [
        "A wrapper function to call the model, and cleanup the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXoGXdD0J_Oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  model_fn = model.signatures['serving_default']\n",
        "  output_dict = model_fn(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key:value[0, :num_detections].numpy() \n",
        "                 for key,value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "   \n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "               image.shape[0], image.shape[1])      \n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    \n",
        "  return output_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtVlOb-h4bOk",
        "colab_type": "text"
      },
      "source": [
        "Run it on each test image and show the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X-EN3hVKxuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_inference(model, image_path, image_name=None):\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = np.array(Image.open(image_path))\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "\n",
        "  if image_name:\n",
        "    plt.imsave(image_name, image_np)\n",
        "  else:\n",
        "    # plt.imshow(image_np)\n",
        "    display(Image.fromarray(image_np))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWWIOqZ7MDbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, image_path in enumerate(TEST_IMAGE_PATHS):\n",
        "  show_inference(detection_model, image_path, image_name=\"gif_frame_\" + ('%02d' % i) + \".jpg\")\n",
        "  # show_inference(detection_model, image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN51SefQM9KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageio.plugins.freeimage.download()\n",
        "\n",
        "anim_file = 'fried-chicken_test.gif'\n",
        "filenames = glob.glob('gif_frame_*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "last = -1\n",
        "images = []\n",
        "for i, filename in enumerate(filenames):\n",
        "  image = imageio.imread(filename)\n",
        "  images.append(image)\n",
        "\n",
        "imageio.mimsave(anim_file, images, 'GIF-FI', fps=5)\n",
        "display(IPyImage(open(anim_file, 'rb').read()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jXXB1j_NJer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_GvRE-x8e5V",
        "colab_type": "text"
      },
      "source": [
        "# Transfer-Learning: Fine-tuning\n",
        "![alt text](https://i.ibb.co/jDZzvML/fine-tuning.png)\n",
        "\n",
        "We'll demonstrate finetuning of a (TF2 friendly) RetinaNet architecture on very few examples of a novel class after initializing from a pre-trained COCO checkpoint.\n",
        "\n",
        "**Important assumption**: patterns extracted in the original dataset are useful in the context of the new dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iem0VKrtkfWi",
        "colab_type": "text"
      },
      "source": [
        "**Goal**: By knowing the background of models (datasets, techniques, etc.) that are used in transfer learning, you can avoid wasting time during experimentation and focus on finetuning models that might make the difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHXuyGxb83b0",
        "colab_type": "text"
      },
      "source": [
        "Utility function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePLjQIW-8uuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: a file path.\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je5SIqbu89DH",
        "colab_type": "text"
      },
      "source": [
        "# Fried-chicken data\n",
        "\n",
        "We will start with some toy data consisting of 10 images of fried-chickken. Note that the [coco](https://cocodataset.org/#explore) dataset contains a number of animals, but notably, it does *not* contain fried-chicken, so this is a novel class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti3Q9U5fOf97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hty-Hnxe85yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load images and visualize\n",
        "train_image_dir = pathlib.Path('MLSS-INDONESIA-2020/dataset/fried-chicken/train')\n",
        "train_image_path = sorted(list(train_image_dir.glob(\"*.jpg\")))\n",
        "\n",
        "train_images_np = []\n",
        "for image_path in train_image_path:\n",
        "  train_images_np.append(load_image_into_numpy_array(str(image_path)))\n",
        "\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['xtick.labelsize'] = False\n",
        "plt.rcParams['ytick.labelsize'] = False\n",
        "plt.rcParams['xtick.top'] = False\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.right'] = False\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "for idx, train_image_np in enumerate(train_images_np):\n",
        "  plt.subplot(2, len(train_images_np)/2, idx+1)\n",
        "  plt.imshow(train_image_np)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPGGvczN_wBZ",
        "colab_type": "text"
      },
      "source": [
        "# Annotate images with bounding boxes\n",
        "\n",
        "In order to train a custom model, we need labelled data. Labelled data in the context of object detection are images with corresponding bounding box coordinates and labels. One convention is to use the top left coordinate and the width and height of the box (x,y,w,h).\n",
        "\n",
        "In this cell you will annotate the fried-chicken --- draw a box around the fried-chicken in each image; click `next image` to go to the next image and `submit` when there are no more images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFm_l7UJ-jD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gt_boxes = []\n",
        "colab_utils.annotate(train_images_np, box_storage_pointer=gt_boxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xld6F0IjARys",
        "colab_type": "text"
      },
      "source": [
        "# Prepare data for training\n",
        "\n",
        "Below we add the class annotations (for simplicity, we assume a single class in this colab; though it should be straightforward to extend this to handle multiple classes).  We also convert everything to the format that the training\n",
        "loop below expects (e.g., everything converted to tensors, classes converted to one-hot representations, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PudtdFbz-wv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# By convention, our non-background classes start counting at 1.  Given\n",
        "# that we will be predicting just one class, we will therefore assign it a\n",
        "# `class id` of 1.\n",
        "fried_chicken_class_id = 1\n",
        "num_classes = 1\n",
        "category_index = {fried_chicken_class_id: {'id': fried_chicken_class_id, 'name': 'fried_chicken'}}\n",
        "\n",
        "# Convert class labels to one-hot; convert everything to tensors.\n",
        "# The `label_id_offset` here shifts all classes by a certain number of indices;\n",
        "# we do this here so that the model receives one-hot labels where non-background\n",
        "# classes start counting at the zeroth index.  This is ordinarily just handled\n",
        "# automatically in our training binaries, but we need to reproduce it here.\n",
        "label_id_offset = 1\n",
        "train_image_tensors = []\n",
        "gt_classes_one_hot_tensors = []\n",
        "gt_box_tensors = []\n",
        "for (train_image_np, gt_box_np) in zip(\n",
        "    train_images_np, gt_boxes):\n",
        "  train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
        "      train_image_np, dtype=tf.float32), axis=0))\n",
        "  gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "  zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
        "      np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
        "  gt_classes_one_hot_tensors.append(tf.one_hot(\n",
        "      zero_indexed_groundtruth_classes, num_classes))\n",
        "print('Done prepping data.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSN_ZweIApOq",
        "colab_type": "text"
      },
      "source": [
        "# Let's just visualize the fried-chicken as a sanity check\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlDT_dSVBPaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "  \"\"\"Wrapper function to visualize detections.\n",
        "\n",
        "  Args:\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "      and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "      this function assumes that the boxes to be plotted are groundtruth\n",
        "      boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "      category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "  \"\"\"\n",
        "  image_np_with_annotations = image_np.copy()\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_annotations,\n",
        "      boxes,\n",
        "      classes,\n",
        "      scores,\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      min_score_thresh=0.8)\n",
        "  if image_name:\n",
        "    plt.imsave(image_name, image_np_with_annotations)\n",
        "  else:\n",
        "    plt.imshow(image_np_with_annotations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEn3GEpvAlUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_scores = np.array([1.0], dtype=np.float32)  # give boxes a score of 100%\n",
        "\n",
        "plt.figure(figsize=(30, 15))\n",
        "for idx in range(len(train_images_np)):\n",
        "  plt.subplot(2, len(train_images_np)/2, idx+1)\n",
        "  plot_detections(\n",
        "      train_images_np[idx],\n",
        "      gt_boxes[idx],\n",
        "      np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
        "      dummy_scores, category_index)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Xw2PrEBjEn",
        "colab_type": "text"
      },
      "source": [
        "# Create model and restore weights for all but last layer\n",
        "\n",
        "In this cell we build a single stage detection architecture (RetinaNet) and restore all but the classification layer at the top (which will be automatically randomly initialized).\n",
        "\n",
        "For simplicity, we have hardcoded a number of things in this colab for the specific RetinaNet architecture at hand (including assuming that the image size will always be 640x640), however it is not difficult to generalize to other model configurations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBENGo9LAzgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
        "\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200710/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa3Q3H-3BsIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print('Building model and restoring weights for fine-tuning...', flush=True)\n",
        "num_classes = 1\n",
        "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "# Load pipeline config and build a detection model.\n",
        "#\n",
        "# Since we are working off of a COCO architecture which predicts 90\n",
        "# class slots by default, we override the `num_classes` field here to be just\n",
        "# one (for our new rubber ducky class).\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "model_config.ssd.num_classes = num_classes\n",
        "model_config.ssd.freeze_batchnorm = True\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=True)\n",
        "\n",
        "# Set up object-based checkpoint restore --- RetinaNet has two prediction\n",
        "# `heads` --- one for classification, the other for box regression.  We will\n",
        "# restore the box regression head but initialize the classification head\n",
        "# from scratch (we show the omission below by commenting out the line that\n",
        "# we would add if we wanted to restore both heads)\n",
        "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
        "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
        "    #    (i.e., the classification head that we *will not* restore)\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )\n",
        "fake_model = tf.compat.v2.train.Checkpoint(\n",
        "          _feature_extractor=detection_model._feature_extractor,\n",
        "          _box_predictor=fake_box_predictor)\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
        "ckpt.restore(checkpoint_path).expect_partial()\n",
        "\n",
        "# Run model through a dummy image so that variables are created\n",
        "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "prediction_dict = detection_model.predict(image, shapes)\n",
        "_ = detection_model.postprocess(prediction_dict, shapes)\n",
        "print('Weights restored!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waBBwIa8B4x1",
        "colab_type": "text"
      },
      "source": [
        "# Eager mode custom training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arfa2se_Bw3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.set_learning_phase(True)\n",
        "\n",
        "# These parameters can be tuned; since our training set has 12 images\n",
        "# it doesn't make sense to have a much larger batch size, though we could\n",
        "# fit more examples in memory if we wanted to.\n",
        "batch_size = 4\n",
        "learning_rate = 0.01\n",
        "num_batches = 100\n",
        "\n",
        "# Select variables in top layers to fine-tune.\n",
        "trainable_variables = detection_model.trainable_variables\n",
        "to_fine_tune = []\n",
        "prefixes_to_train = [\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
        "for var in trainable_variables:\n",
        "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "    to_fine_tune.append(var)\n",
        "\n",
        "# Set up forward + backward pass for a single train step.\n",
        "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
        "  \"\"\"Get a tf.function for training step.\"\"\"\n",
        "\n",
        "  # Use tf.function for a bit of speed.\n",
        "  # Comment out the tf.function decorator if you want the inside of the\n",
        "  # function to run eagerly.\n",
        "  @tf.function\n",
        "  def train_step_fn(image_tensors,\n",
        "                    groundtruth_boxes_list,\n",
        "                    groundtruth_classes_list):\n",
        "    \"\"\"A single training iteration.\n",
        "\n",
        "    Args:\n",
        "      image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "        Note that the height and width can vary across images, as they are\n",
        "        reshaped within this function to be 640x640.\n",
        "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "        with type tf.float32 representing groundtruth boxes for each image in\n",
        "        the batch.\n",
        "\n",
        "    Returns:\n",
        "      A scalar tensor representing the total loss for the input batch.\n",
        "    \"\"\"\n",
        "    shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
        "    model.provide_groundtruth(\n",
        "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "        groundtruth_classes_list=groundtruth_classes_list)\n",
        "    with tf.GradientTape() as tape:\n",
        "      preprocessed_images = tf.concat(\n",
        "          [detection_model.preprocess(image_tensor)[0]\n",
        "           for image_tensor in image_tensors], axis=0)\n",
        "      prediction_dict = model.predict(preprocessed_images, shapes)\n",
        "      losses_dict = model.loss(prediction_dict, shapes)\n",
        "      total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "      gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "      optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "    return total_loss\n",
        "\n",
        "  return train_step_fn\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "train_step_fn = get_model_train_step_function(\n",
        "    detection_model, optimizer, to_fine_tune)\n",
        "\n",
        "print('Start fine-tuning!', flush=True)\n",
        "for idx in range(num_batches):\n",
        "  # Grab keys for a random subset of examples\n",
        "  all_keys = list(range(len(train_images_np)))\n",
        "  random.shuffle(all_keys)\n",
        "  example_keys = all_keys[:batch_size]\n",
        "\n",
        "  # Note that we do not do data augmentation in this demo.  If you want a\n",
        "  # a fun exercise, we recommend experimenting with random horizontal flipping\n",
        "  # and random cropping :)\n",
        "  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
        "  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
        "  image_tensors = [train_image_tensors[key] for key in example_keys]\n",
        "\n",
        "  # Training step (forward pass + backwards pass)\n",
        "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
        "\n",
        "  if idx % 10 == 0:\n",
        "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
        "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
        "\n",
        "print('Done fine-tuning!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTLDG1MtGwRv",
        "colab_type": "text"
      },
      "source": [
        "# Load test images and run inference with new model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAB2orKbB8U6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_image_dir = pathlib.Path('MLSS-INDONESIA-2020/dataset/fried-chicken/test')\n",
        "train_image_path = sorted(list(test_image_dir.glob(\"*.jpg\")))\n",
        "\n",
        "test_images_np = []\n",
        "for image_path in train_image_path:\n",
        "  test_images_np.append(np.expand_dims(\n",
        "      load_image_into_numpy_array(str(image_path)), axis=0))\n",
        "\n",
        "# Again, uncomment this decorator if you want to run inference eagerly\n",
        "@tf.function\n",
        "def detect(input_tensor):\n",
        "  \"\"\"Run detection on an input image.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "      Note that height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "\n",
        "  Returns:\n",
        "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
        "      and `detection_scores`).\n",
        "  \"\"\"\n",
        "  preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "  prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "  return detection_model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "# Note that the first frame will trigger tracing of the tf.function, which will\n",
        "# take some time, after which inference should be fast.\n",
        "\n",
        "label_id_offset = 1\n",
        "for i in range(len(test_images_np)):\n",
        "  input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
        "  detections = detect(input_tensor)\n",
        "\n",
        "  plot_detections(\n",
        "      test_images_np[i][0],\n",
        "      detections['detection_boxes'][0].numpy(),\n",
        "      detections['detection_classes'][0].numpy().astype(np.uint32)\n",
        "      + label_id_offset,\n",
        "      detections['detection_scores'][0].numpy(),\n",
        "      category_index, figsize=(15, 20), image_name=\"gif_frame_\" + ('%02d' % i) + \".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sc0yi7kHiVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageio.plugins.freeimage.download()\n",
        "\n",
        "anim_file = 'fried-chicken_test.gif'\n",
        "filenames = glob.glob('gif_frame_*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "last = -1\n",
        "images = []\n",
        "for i, filename in enumerate(filenames):\n",
        "  image = imageio.imread(filename)\n",
        "  images.append(image)\n",
        "\n",
        "imageio.mimsave(anim_file, images, 'GIF-FI', fps=5)\n",
        "display(IPyImage(open(anim_file, 'rb').read()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHTZDefFH82x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}