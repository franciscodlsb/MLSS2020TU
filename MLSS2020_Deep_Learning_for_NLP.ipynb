{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLSS2020: Deep Learning for NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LU2JF581rNLG",
        "LXyNrjm5q_Ox",
        "LksfFmg49WlJ",
        "6OC3iS43ceN0",
        "0AExfe5_d7hj",
        "xBlKA0qtg_XJ",
        "Bo32b2KmnXKC",
        "vAOOi77horK0",
        "TDfEDcFz7WrX",
        "EAmOCyG5kCFj",
        "HOrF4_MChve6",
        "BYKJS9YL9CEr",
        "Vs9rRxOyMvMu",
        "t5Rt03So6REP",
        "tXpcQQwK6WTd",
        "3zAMDzLBrI6r",
        "WXS9SWVn6a2h",
        "_grtGge7vNax",
        "Z5rpez6yMyXw",
        "kxLKI-uFwC62",
        "LCZUaqCD6glA"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franciscodlsb/MLSS2020TU/blob/master/MLSS2020_Deep_Learning_for_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqcEIaQXgXT2",
        "colab_type": "text"
      },
      "source": [
        "# **Practical 6: Deep Learning for NLP**\n",
        "for Machine Learning Summer School (MLSS) 2020 by Genta Indra Winata.\n",
        "\n",
        "https://mlss.telkomuniversity.ac.id/\n",
        "\n",
        "This tutorial is divided into three main sections:\n",
        "1. Implement a simple neural network with a single linear layer and an embedding layer trained from scratch\n",
        "2. Leverage pre-trained word embeddings for transfer learning.\n",
        "3. Explore techniques to improve the model's robustness and generalization.\n",
        "\n",
        "We will use an existing IMDB sentiment text analysis task. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Qzc-BxcnOC",
        "colab_type": "text"
      },
      "source": [
        "## Train from scratch\n",
        "\n",
        "First, we import classes from PyTorch library and define hyper-parameters we will use to train our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBqNvSmtgmsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b46be7c1-fd74-44b9-ac50-fac3e1ebded7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"device: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.6.0+cu101\n",
            "device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUcs5cKehGII",
        "colab_type": "text"
      },
      "source": [
        "Define the hyper-parameters here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PWBjtdQsOPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "args = {\n",
        "  \"hidden_size\": 100,\n",
        "  \"output_size\": 1,\n",
        "  \"lr\": 1e-4,\n",
        "  \"seed\": 1234,\n",
        "  \"max_vocab_size\": 1000,\n",
        "  \"batch_size\": 64,\n",
        "  \"num_epoch\": 10,\n",
        "  \"dropout\": 0.0\n",
        "}\n",
        "args = namedtuple('Struct', args.keys())(*args.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3enVbGCijF2",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing\n",
        "We will load the IMDB sentiment analysis dataset by simply downloading the data from `torchtext` package.\n",
        "Using `data.Field`, we specify the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN1DSXdfil3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import datasets\n",
        "from torchtext import data\n",
        "import random\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True) # add spacy tokenizer\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(args.seed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n_UIYF_a_Dt",
        "colab_type": "text"
      },
      "source": [
        "We can check the data split and print a training sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po34C224ln6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "f49ad9d6-c80c-48b4-9886-3ef4a1d8f622"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of valid examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')\n",
        "print(f\"Text: {vars(train_data.examples[0])['text']}\")\n",
        "print(f\"Label: {vars(train_data.examples[0])['label']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of valid examples: 7500\n",
            "Number of testing examples: 25000\n",
            "Text: ['This', 'film', 'is', 'the', 'freshman', 'effort', 'of', 'Stephanie', 'Beaton', 'and', 'her', 'new', 'production', 'company', '.', 'While', 'it', 'suffers', 'from', 'a', 'few', 'problems', ',', 'as', 'every', 'low', 'budget', 'production', 'does', ',', 'it', 'is', 'a', 'good', 'start', 'for', 'Ms.', 'Beaton', 'and', 'her', 'company.<br', '/><br', '/>The', 'story', 'is', 'not', 'terribly', 'new', 'having', 'been', 'done', 'in', 'films', 'like', 'The', 'Burning', 'and', 'every', 'Friday', 'the', '13th', 'since', 'part', '2', '.', 'But', ',', 'the', 'performances', 'are', 'heartfelt', '.', 'So', 'many', 'big', 'budget', 'movies', 'just', 'have', 'the', 'actors', 'going', 'through', 'the', 'motions', ',', 'its', 'always', 'nice', 'to', 'see', 'actors', 'really', 'trying', 'to', 'hone', 'their', 'craft.<br', '/><br', '/>The', 'story', 'deals', 'with', 'the', 'murder(and', 'possible', 'return', ')', 'of', 'a', 'disfigured', 'classmate', '.', 'The', 'others', 'are', 'sworn', 'to', 'secrecy', ',', 'but', 'the', 'trauma', 'of', 'the', 'event', 'sends', 'each', 'person', 'in', 'different', 'directions', 'in', 'their', 'lifes', '.', 'Ten', 'years', 'later', ',', 'the', 'friends', 'are', 'murdered', 'one', 'by', 'one', 'by', 'a', 'gruesome', 'stalker', 'known', 'as', '\"', 'The', 'Bagman', '\"', '.', 'Who', 'will', 'survive', '?', 'You', 'have', 'to', 'watch.<br', '/><br', '/>If', 'you', 'are', 'Roger', 'Ebert', 'or', 'any', 'number', 'of', 'arrogant', 'critics', ',', 'you', 'probably', 'should', \"n't\", 'bother', '.', 'But', 'if', 'your', 'taste', 'run', 'more', 'towards', 'Joe', 'Bob', 'Briggs', 'and', 'you', 'want', 'to', 'see', 'a', 'group', 'of', 'people', 'honing', 'their', 'craft', ',', 'then', 'check', 'out', '\"', 'The', 'Bagman', '\"', '.']\n",
            "Label: neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRQPGa0JpQkk",
        "colab_type": "text"
      },
      "source": [
        "We can also utilize the `torchtext` field classes for building the vocabulary and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhGIlvZfmAiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train_data, max_size=args.max_vocab_size)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gssNGNYmEDg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a2f1c030-be8b-4acf-b4c2-366505728f66"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 1002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP9-OgyshECE",
        "colab_type": "text"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRzurgQ1hMWp",
        "colab_type": "text"
      },
      "source": [
        "Let's define a linear layer model. This model consists of an embedding layer and a linear layer as the decoder.\n",
        "\n",
        "Padding:\n",
        "I love cat = [1 2 3]\n",
        "I love black cat = [1 2 4 3]\n",
        "I love = [1 2]\n",
        "[\n",
        "[1 2 4 3]\n",
        "[1 2 3 0]\n",
        "[1 2 0 0]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4bIMH_MhUyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SingleNNLayer(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, output_size, dropout=0.0, pad_idx=0):\n",
        "    super(SingleNNLayer, self).__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_idx)\n",
        "    self.layer = nn.Linear(hidden_size, output_size)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, inputs, inputs_len):\n",
        "    \"\"\"\n",
        "      inputs: LongTensor (seq_len, batch_size)\n",
        "    \"\"\"\n",
        "    inputs = inputs.transpose(0, 1) # (batch_size, seq_len)\n",
        "    embedded_inputs = self.drop(self.emb(inputs)) # (batch_size, seq_len, emb_size)\n",
        "    pooled_inputs = F.avg_pool2d(embedded_inputs, (embedded_inputs.shape[1], 1)).squeeze(1)  # (batch_size, emb_size)\n",
        "    outputs = self.drop(self.layer(pooled_inputs)) # (batch_size, output_size)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXqxn1d2hROo",
        "colab_type": "text"
      },
      "source": [
        "We use accuracy as our metric to evaluate our model on the test data. We apply sigmoid as the activation function, if the prediction is higher or equal to 0.5, it classifies as positive, otherwise negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3kmNZR7le8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch\n",
        "    \"\"\"\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds)) # threshold 0.5\n",
        "    correct = (rounded_preds == y).float() # convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6rX3irkdDTy",
        "colab_type": "text"
      },
      "source": [
        "We define the `train` function. The function uses to sample each batch from the iterator. \n",
        "\n",
        "`optimizer.zero_grad()` is called to zero out the gradients.\n",
        "Then, we compute the loss and accuracy average."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCYmiaPprNUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, text_lengths = batch.text\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        loss.backward() # compute gradient\n",
        "        optimizer.step() # update parameters\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReYA1N3kd5OA",
        "colab_type": "text"
      },
      "source": [
        "We define the `evaluate` function to compute the loss and accuracy of the test set. \n",
        "\n",
        "`model.eval` is called to remove all dropouts.\n",
        "\n",
        "`torch.no_grad()` is called to ignore the gradient computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KRVsKDN2G3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    \n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.text\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op8EycnBejKX",
        "colab_type": "text"
      },
      "source": [
        "We define the `train_model` to start our training for `args.num_epoch` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6spa3fL0hqhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def train_model(model, train_iterator, valid_iterator, saved_name=\"best_model.pt\", optimizer=None):\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  if optimizer == None:\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "  model.to(device)\n",
        "  criterion.to(device)\n",
        "\n",
        "  best_valid_loss = float('inf')\n",
        "  for i in range(args.num_epoch):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    end_time = time.time()\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator)\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    print(f\"Epoch: {i+1} train loss:{train_loss:.3f} acc:{train_acc:.3f} valid loss:{valid_loss:.3f} acc:{valid_acc:.3f} time:{elapsed_time:.3f}s\")\n",
        "\n",
        "    # Choose the best valid loss\n",
        "    if best_valid_loss > valid_loss:\n",
        "      torch.save(model.state_dict(), saved_name)\n",
        "      best_valid_loss = valid_loss\n",
        "      print(\"Save model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU2JF581rNLG",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "The data is split into train, valid, and test.\n",
        "Each batch is sorted according to the sequence length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9goiXQ7xkkRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = args.batch_size, \n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VYCR2PxfIgQ",
        "colab_type": "text"
      },
      "source": [
        "Let's create our model and start the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcgFrss-1IMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "5a565a4c-1d18-4f46-b664-05ccd66d0094"
      },
      "source": [
        "# Model to train\n",
        "model = SingleNNLayer(len(TEXT.vocab), args.hidden_size, args.output_size)\n",
        "train_model(model, train_iterator, valid_iterator, \"best_model.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 train loss:0.672 acc:0.639 valid loss:0.644 acc:0.697 time:1.686s\n",
            "Save model\n",
            "Epoch: 2 train loss:0.592 acc:0.740 valid loss:0.550 acc:0.755 time:1.616s\n",
            "Save model\n",
            "Epoch: 3 train loss:0.499 acc:0.787 valid loss:0.478 acc:0.794 time:1.584s\n",
            "Save model\n",
            "Epoch: 4 train loss:0.439 acc:0.816 valid loss:0.433 acc:0.815 time:1.609s\n",
            "Save model\n",
            "Epoch: 5 train loss:0.402 acc:0.833 valid loss:0.407 acc:0.827 time:1.597s\n",
            "Save model\n",
            "Epoch: 6 train loss:0.379 acc:0.844 valid loss:0.391 acc:0.832 time:1.601s\n",
            "Save model\n",
            "Epoch: 7 train loss:0.364 acc:0.849 valid loss:0.381 acc:0.838 time:1.613s\n",
            "Save model\n",
            "Epoch: 8 train loss:0.352 acc:0.855 valid loss:0.375 acc:0.840 time:1.595s\n",
            "Save model\n",
            "Epoch: 9 train loss:0.343 acc:0.860 valid loss:0.368 acc:0.842 time:1.599s\n",
            "Save model\n",
            "Epoch: 10 train loss:0.337 acc:0.863 valid loss:0.364 acc:0.845 time:1.574s\n",
            "Save model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXyNrjm5q_Ox",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation\n",
        "We evaluate our best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSkKkIPZXXDj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0351e70-2998-49f4-9aef-d8be5577f409"
      },
      "source": [
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iterator)\n",
        "print(f\"train loss:{test_loss:.3f} acc:{test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:0.354 acc:0.850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SDEsznIaiWR",
        "colab_type": "text"
      },
      "source": [
        "## Transfer Learning with Pre-trained Word Embeddings\n",
        "\n",
        "We will use pre-trained GLoVe word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LksfFmg49WlJ",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing\n",
        "We load `glove.6B.100d` embeddings from the `torchtext` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjzwSP6HaoLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train_data, \n",
        "  max_size = args.max_vocab_size, \n",
        "  vectors = \"glove.6B.100d\", \n",
        "  unk_init = torch.Tensor.normal_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCgwVvBAarFv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55db7254-869f-4301-b252-cde8f7e3e916"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 1002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OC3iS43ceN0",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "We will set the `PAD_IDX` and `UNK_IDX` to the model's embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i29bNsMUax7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "7bd39588-8ae1-4b4d-8b65-45b767e92674"
      },
      "source": [
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "# Load pretrained word embeddings\n",
        "glove_model = SingleNNLayer(len(TEXT.vocab), args.hidden_size, args.output_size, pad_idx=PAD_IDX)\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "glove_model.emb.weight.data.copy_(pretrained_embeddings)\n",
        "glove_model.emb.weight.data[UNK_IDX] = torch.zeros(args.hidden_size)\n",
        "glove_model.emb.weight.data[PAD_IDX] = torch.zeros(args.hidden_size)\n",
        "\n",
        "train_model(glove_model, train_iterator, valid_iterator, \"best_glove_model.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 train loss:0.673 acc:0.639 valid loss:0.639 acc:0.723 time:1.497s\n",
            "Save model\n",
            "Epoch: 2 train loss:0.593 acc:0.741 valid loss:0.537 acc:0.781 time:1.459s\n",
            "Save model\n",
            "Epoch: 3 train loss:0.496 acc:0.798 valid loss:0.458 acc:0.816 time:1.478s\n",
            "Save model\n",
            "Epoch: 4 train loss:0.432 acc:0.823 valid loss:0.416 acc:0.832 time:1.467s\n",
            "Save model\n",
            "Epoch: 5 train loss:0.396 acc:0.839 valid loss:0.394 acc:0.835 time:1.507s\n",
            "Save model\n",
            "Epoch: 6 train loss:0.373 acc:0.845 valid loss:0.381 acc:0.840 time:1.493s\n",
            "Save model\n",
            "Epoch: 7 train loss:0.358 acc:0.851 valid loss:0.374 acc:0.840 time:1.503s\n",
            "Save model\n",
            "Epoch: 8 train loss:0.347 acc:0.856 valid loss:0.366 acc:0.845 time:1.454s\n",
            "Save model\n",
            "Epoch: 9 train loss:0.338 acc:0.861 valid loss:0.363 acc:0.846 time:1.454s\n",
            "Save model\n",
            "Epoch: 10 train loss:0.332 acc:0.863 valid loss:0.362 acc:0.846 time:1.492s\n",
            "Save model\n",
            "Epoch: 11 train loss:0.326 acc:0.866 valid loss:0.359 acc:0.848 time:1.467s\n",
            "Save model\n",
            "Epoch: 12 train loss:0.322 acc:0.867 valid loss:0.358 acc:0.849 time:1.517s\n",
            "Save model\n",
            "Epoch: 13 train loss:0.318 acc:0.870 valid loss:0.360 acc:0.846 time:1.475s\n",
            "Epoch: 14 train loss:0.315 acc:0.870 valid loss:0.358 acc:0.851 time:1.470s\n",
            "Epoch: 15 train loss:0.313 acc:0.871 valid loss:0.359 acc:0.849 time:1.489s\n",
            "Epoch: 16 train loss:0.311 acc:0.872 valid loss:0.358 acc:0.849 time:1.479s\n",
            "Epoch: 17 train loss:0.309 acc:0.873 valid loss:0.359 acc:0.849 time:1.449s\n",
            "Epoch: 18 train loss:0.307 acc:0.872 valid loss:0.359 acc:0.852 time:1.452s\n",
            "Epoch: 19 train loss:0.305 acc:0.873 valid loss:0.360 acc:0.851 time:1.460s\n",
            "Epoch: 20 train loss:0.304 acc:0.874 valid loss:0.361 acc:0.851 time:1.460s\n",
            "Epoch: 21 train loss:0.304 acc:0.875 valid loss:0.361 acc:0.851 time:1.483s\n",
            "Epoch: 22 train loss:0.303 acc:0.876 valid loss:0.363 acc:0.851 time:1.468s\n",
            "Epoch: 23 train loss:0.302 acc:0.875 valid loss:0.362 acc:0.852 time:1.470s\n",
            "Epoch: 24 train loss:0.301 acc:0.876 valid loss:0.364 acc:0.851 time:1.467s\n",
            "Epoch: 25 train loss:0.301 acc:0.877 valid loss:0.364 acc:0.852 time:1.467s\n",
            "Epoch: 26 train loss:0.301 acc:0.876 valid loss:0.365 acc:0.852 time:1.477s\n",
            "Epoch: 27 train loss:0.299 acc:0.877 valid loss:0.366 acc:0.851 time:1.466s\n",
            "Epoch: 28 train loss:0.299 acc:0.878 valid loss:0.367 acc:0.852 time:1.470s\n",
            "Epoch: 29 train loss:0.299 acc:0.878 valid loss:0.366 acc:0.852 time:1.464s\n",
            "Epoch: 30 train loss:0.298 acc:0.877 valid loss:0.372 acc:0.849 time:1.471s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AExfe5_d7hj",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ATbZ0dma1FZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2eb1a41-ffdf-4e7a-c354-05b15a9ee6c0"
      },
      "source": [
        "glove_model.load_state_dict(torch.load('best_glove_model.pt'))\n",
        "test_loss, test_acc = evaluate(glove_model, test_iterator)\n",
        "print(f\"test loss:{test_loss:.3f} acc:{test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss:0.344 acc:0.856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqspM3ICgZCu",
        "colab_type": "text"
      },
      "source": [
        "## Improving the Model's Robustness and Generalization\n",
        "\n",
        "We can apply simple techniques to improve the generalization of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBlKA0qtg_XJ",
        "colab_type": "text"
      },
      "source": [
        "### Adding Dropout\n",
        "Dropout: A Simple Way to Prevent Neural Networks from Overfitting \n",
        "http://jmlr.org/papers/v15/srivastava14a.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91eCJlo1g1R1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "d9722a1f-c5c2-42b3-91d7-6d9332e6ef74"
      },
      "source": [
        "# Load pretrained word embeddings\n",
        "glove_model = SingleNNLayer(len(TEXT.vocab), args.hidden_size, args.output_size, pad_idx=PAD_IDX, dropout=0.2)\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "glove_model.emb.weight.data.copy_(pretrained_embeddings)\n",
        "glove_model.emb.weight.data[UNK_IDX] = torch.zeros(args.hidden_size)\n",
        "glove_model.emb.weight.data[PAD_IDX] = torch.zeros(args.hidden_size)\n",
        "train_model(glove_model, train_iterator, valid_iterator, \"best_glove_model_with_dropout.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 train loss:0.676 acc:0.613 valid loss:0.648 acc:0.713 time:1.527s\n",
            "Save model\n",
            "Epoch: 2 train loss:0.615 acc:0.691 valid loss:0.565 acc:0.780 time:1.508s\n",
            "Save model\n",
            "Epoch: 3 train loss:0.541 acc:0.736 valid loss:0.491 acc:0.813 time:1.505s\n",
            "Save model\n",
            "Epoch: 4 train loss:0.491 acc:0.759 valid loss:0.447 acc:0.825 time:1.503s\n",
            "Save model\n",
            "Epoch: 5 train loss:0.462 acc:0.766 valid loss:0.421 acc:0.831 time:1.511s\n",
            "Save model\n",
            "Epoch: 6 train loss:0.443 acc:0.775 valid loss:0.402 acc:0.834 time:1.530s\n",
            "Save model\n",
            "Epoch: 7 train loss:0.429 acc:0.777 valid loss:0.391 acc:0.838 time:1.515s\n",
            "Save model\n",
            "Epoch: 8 train loss:0.420 acc:0.786 valid loss:0.381 acc:0.844 time:1.519s\n",
            "Save model\n",
            "Epoch: 9 train loss:0.415 acc:0.783 valid loss:0.375 acc:0.844 time:1.529s\n",
            "Save model\n",
            "Epoch: 10 train loss:0.408 acc:0.786 valid loss:0.371 acc:0.846 time:1.515s\n",
            "Save model\n",
            "Epoch: 11 train loss:0.404 acc:0.793 valid loss:0.367 acc:0.846 time:1.504s\n",
            "Save model\n",
            "Epoch: 12 train loss:0.399 acc:0.793 valid loss:0.366 acc:0.846 time:1.513s\n",
            "Save model\n",
            "Epoch: 13 train loss:0.399 acc:0.793 valid loss:0.367 acc:0.846 time:1.501s\n",
            "Epoch: 14 train loss:0.394 acc:0.794 valid loss:0.363 acc:0.848 time:1.496s\n",
            "Save model\n",
            "Epoch: 15 train loss:0.390 acc:0.798 valid loss:0.364 acc:0.847 time:1.495s\n",
            "Epoch: 16 train loss:0.393 acc:0.795 valid loss:0.363 acc:0.848 time:1.491s\n",
            "Epoch: 17 train loss:0.392 acc:0.800 valid loss:0.360 acc:0.850 time:1.528s\n",
            "Save model\n",
            "Epoch: 18 train loss:0.389 acc:0.798 valid loss:0.359 acc:0.851 time:1.503s\n",
            "Save model\n",
            "Epoch: 19 train loss:0.385 acc:0.797 valid loss:0.367 acc:0.846 time:1.501s\n",
            "Epoch: 20 train loss:0.384 acc:0.799 valid loss:0.360 acc:0.850 time:1.508s\n",
            "Epoch: 21 train loss:0.384 acc:0.799 valid loss:0.361 acc:0.850 time:1.500s\n",
            "Epoch: 22 train loss:0.384 acc:0.801 valid loss:0.359 acc:0.851 time:1.497s\n",
            "Save model\n",
            "Epoch: 23 train loss:0.384 acc:0.801 valid loss:0.359 acc:0.851 time:1.527s\n",
            "Epoch: 24 train loss:0.382 acc:0.796 valid loss:0.359 acc:0.851 time:1.535s\n",
            "Save model\n",
            "Epoch: 25 train loss:0.383 acc:0.802 valid loss:0.358 acc:0.851 time:1.497s\n",
            "Save model\n",
            "Epoch: 26 train loss:0.388 acc:0.795 valid loss:0.361 acc:0.851 time:1.501s\n",
            "Epoch: 27 train loss:0.379 acc:0.803 valid loss:0.359 acc:0.851 time:1.497s\n",
            "Epoch: 28 train loss:0.386 acc:0.798 valid loss:0.359 acc:0.852 time:1.511s\n",
            "Epoch: 29 train loss:0.378 acc:0.802 valid loss:0.359 acc:0.853 time:1.491s\n",
            "Epoch: 30 train loss:0.380 acc:0.802 valid loss:0.360 acc:0.852 time:1.519s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsQcglrfhaFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d652577-708c-49bb-baa2-112c13195085"
      },
      "source": [
        "glove_model.load_state_dict(torch.load('best_glove_model_with_dropout.pt'))\n",
        "test_loss, test_acc = evaluate(glove_model, test_iterator)\n",
        "print(f\"test loss:{test_loss:.3f} acc:{test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss:0.341 acc:0.859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo32b2KmnXKC",
        "colab_type": "text"
      },
      "source": [
        "### Adding a non-linear function\n",
        "\n",
        "We can add a non-linear function between layers to introduce a non-linearity to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG1c1vaum-58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SingleNNLayerWithRELU(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, output_size, dropout=0.0, pad_idx=0):\n",
        "    super(SingleNNLayerWithRELU, self).__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_idx)\n",
        "    self.layer = nn.Linear(hidden_size, output_size)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, inputs, inputs_len):\n",
        "    \"\"\"\n",
        "      inputs: LongTensor (seq_len, batch_size)\n",
        "    \"\"\"\n",
        "    inputs = inputs.transpose(0, 1)\n",
        "    embedded_inputs = self.drop(self.emb(inputs)) # (batch_size, seq_len, emb_size)\n",
        "    pooled_inputs = F.avg_pool2d(embedded_inputs, (embedded_inputs.shape[1], 1)).squeeze(1)  # (batch_size, emb_size)\n",
        "    outputs = self.drop(self.layer(F.relu(pooled_inputs))) # (batch_size, output_size)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dtNJVf_nfeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "28840645-88d8-4258-bc23-f2ed136ead70"
      },
      "source": [
        "# Load pretrained word embeddings\n",
        "glove_model = SingleNNLayerWithRELU(len(TEXT.vocab), args.hidden_size, args.output_size, pad_idx=PAD_IDX, dropout=0.3)\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "glove_model.emb.weight.data.copy_(pretrained_embeddings)\n",
        "glove_model.emb.weight.data[UNK_IDX] = torch.zeros(args.hidden_size)\n",
        "glove_model.emb.weight.data[PAD_IDX] = torch.zeros(args.hidden_size)\n",
        "train_model(glove_model, train_iterator, valid_iterator, \"best_glove_model_with_relu.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 train loss:0.691 acc:0.532 valid loss:0.668 acc:0.613\n",
            "Save model\n",
            "Epoch: 2 train loss:0.675 acc:0.609 valid loss:0.597 acc:0.706\n",
            "Save model\n",
            "Epoch: 3 train loss:0.648 acc:0.650 valid loss:0.528 acc:0.747\n",
            "Save model\n",
            "Epoch: 4 train loss:0.617 acc:0.679 valid loss:0.480 acc:0.774\n",
            "Save model\n",
            "Epoch: 5 train loss:0.589 acc:0.704 valid loss:0.443 acc:0.798\n",
            "Save model\n",
            "Epoch: 6 train loss:0.565 acc:0.718 valid loss:0.425 acc:0.815\n",
            "Save model\n",
            "Epoch: 7 train loss:0.543 acc:0.725 valid loss:0.424 acc:0.823\n",
            "Save model\n",
            "Epoch: 8 train loss:0.527 acc:0.731 valid loss:0.418 acc:0.829\n",
            "Save model\n",
            "Epoch: 9 train loss:0.515 acc:0.738 valid loss:0.421 acc:0.834\n",
            "Epoch: 10 train loss:0.504 acc:0.739 valid loss:0.427 acc:0.835\n",
            "Epoch: 11 train loss:0.492 acc:0.743 valid loss:0.437 acc:0.839\n",
            "Epoch: 12 train loss:0.486 acc:0.744 valid loss:0.442 acc:0.842\n",
            "Epoch: 13 train loss:0.477 acc:0.744 valid loss:0.452 acc:0.845\n",
            "Epoch: 14 train loss:0.477 acc:0.745 valid loss:0.457 acc:0.848\n",
            "Epoch: 15 train loss:0.473 acc:0.749 valid loss:0.461 acc:0.848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc3S3W_1nfsM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0d93003-ae84-4e19-9f56-509741dba5bf"
      },
      "source": [
        "glove_model.load_state_dict(torch.load('best_glove_model_with_relu.pt'))\n",
        "test_loss, test_acc = evaluate(model_with_relu, test_iterator, criterion)\n",
        "print(f\"test loss:{test_loss:.3f} acc:{test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss:0.432 acc:0.821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEE1-VP5V6iR",
        "colab_type": "text"
      },
      "source": [
        "# Practical 7: RNNs and Transformers\n",
        "\n",
        "The tutorial is divided into few sections: \n",
        "1. Implement an LSTM-based language model on English Penn Tree Bank data\n",
        "2. Implement an LSTM and a Transformer model for the imdb sentiment sequence classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOBzleBkzRP_",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Language Model\n",
        "\n",
        "References:\n",
        "\n",
        "Some parts of the code are taken from https://github.com/salesforce/awd-lstm-lm/ and https://github.com/gentaiscool/multi-task-cs-lm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAOOi77horK0",
        "colab_type": "text"
      },
      "source": [
        "### Download dataset\n",
        "English Penn Tree Bank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPqpjQumz13G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "TRAIN_PATH = \"https://raw.githubusercontent.com/tmatha/lstm/master/ptb.train.txt\"\n",
        "VALID_PATH = \"https://raw.githubusercontent.com/tmatha/lstm/master/ptb.valid.txt\"\n",
        "TEST_PATH = \"https://raw.githubusercontent.com/tmatha/lstm/master/ptb.test.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDfEDcFz7WrX",
        "colab_type": "text"
      },
      "source": [
        "### Build the model\n",
        "Let's define our model `RNNModel`. You can set the model to instantiate `RNN`, `LSTM` or `GRU`. You can use `tied weights` for sharing the same parameters on both input and output embeddings' weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MWM0yywy5Zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError:\n",
        "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "        self.tie_weights = tie_weights\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        \n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "\n",
        "        decoded = F.log_softmax(self.decoder(output.view(output.size(0)*output.size(1), output.size(2))))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
        "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR0DjMPHftVh",
        "colab_type": "text"
      },
      "source": [
        "We define the `Dictionary` class. This helper class stores the map from words to their indices, and vice versa in `word2idx` and `idx2word`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0X6RsPE7w_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "class Dictionary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {} # {\"apple\": 1, \"hello\": 2}\n",
        "        self.idx2word = {} # {1 : \"apple\", 2: \"hello\"}\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word[len(self.idx2word)] = word\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iSbWrOe7-Cn",
        "colab_type": "text"
      },
      "source": [
        "This is the `Corpus` class to store our preprocessed train, valid, and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpyp9rTy7NE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Corpus:\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(TRAIN_PATH)\n",
        "        print(\"train:\", len(self.dictionary))\n",
        "        self.valid = self.tokenize(VALID_PATH)\n",
        "        print(\"valid:\", len(self.dictionary))\n",
        "        self.test = self.tokenize(TEST_PATH)\n",
        "        print(\"test:\", len(self.dictionary))\n",
        "        print(\"dictionary size:\", len(self.dictionary))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        self.dictionary.add_word(\"<oov>\")\n",
        "\n",
        "        with urllib.request.urlopen(path) as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                line = line.strip().decode(\"utf-8\")\n",
        "                line = line.replace(\"  \", \" \")\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with urllib.request.urlopen(path) as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                line = line.decode(\"utf-8\")\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAmOCyG5kCFj",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-9ml9OY2mVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "230705c6-8b27-44da-f9be-37ca476e7005"
      },
      "source": [
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import unicodedata\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "from collections import namedtuple\n",
        "\n",
        "args = {\n",
        "    \"name\": \"name\",\n",
        "    \"model\": \"LSTM\",\n",
        "    \"emsize\": 200,\n",
        "    \"nhid\": 200,\n",
        "    \"nlayers\": 2,\n",
        "    \"lr\": 20,\n",
        "    \"clip\": 0.25,\n",
        "    \"epochs\": 20,\n",
        "    \"batch_size\": 20,\n",
        "    \"bptt\": 35,\n",
        "    \"dropout\": 0.2,\n",
        "    \"tied\": False,\n",
        "    \"pad\": True,\n",
        "    \"seed\": 1234,\n",
        "    \"cuda\": True,\n",
        "    \"save\": \".\",\n",
        "    \"log_path\": \".\",\n",
        "    \"log_interval\": 200\n",
        "}\n",
        "\n",
        "args = namedtuple('Struct', args.keys())(*args.values())\n",
        "log_name = str(args.name) + \"_model\" + str(args.model) + \"_layers\" + str(args.nlayers) + \"_nhid\" + str(args.nhid) + \"_emsize\" + str(args.emsize) + \".txt\"\n",
        "log_file = open(args.log_path + \"/\" + log_name, \"w+\")\n",
        "\n",
        "save_path = args.save + \"/\" + log_name + \".pt\"\n",
        "\n",
        "is_pad = False\n",
        "if args.pad:\n",
        "    is_pad = args.pad\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Load data\n",
        "corpus = Corpus()\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    if args.cuda:\n",
        "        data = data.cuda()\n",
        "    return data\n",
        "\n",
        "\n",
        "eval_batch_size = 32\n",
        "\n",
        "train_data = batchify(corpus.train, args.batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "# Build the model\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied)\n",
        "print(model)\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "\n",
        "# Training code\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors,\n",
        "    to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def get_batch(source, i, evaluation=False):\n",
        "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "word2idx = corpus.dictionary.word2idx\n",
        "idx2word = corpus.dictionary.idx2word\n",
        "num_word = len(corpus.dictionary.idx2word)\n",
        "\n",
        "def evaluate(data_source, type_evaluation=\"val\"):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "        data, targets = get_batch(data_source, i, evaluation=True)\n",
        "        output, hidden = model(data, hidden)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        total_loss += len(data) * criterion(output_flat, targets).data\n",
        "        hidden = repackage_hidden(hidden)\n",
        "    return total_loss.item() / len(data_source)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(args.batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    batch_idx = 0\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        \n",
        "        output, hidden = model(data, hidden)\n",
        "\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        batch_idx += data.size(1)\n",
        "\n",
        "        # clip the grad\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
        "        opt = optim.SGD(model.parameters(), lr=lr)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.data\n",
        "        \n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss.item() / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            log = '| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | word_loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args.bptt, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss))\n",
        "            print(log)\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "lr = args.lr\n",
        "best_val_loss = None\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(1, args.epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(val_data, \"dev\")\n",
        "\n",
        "    log = '-' * 89 + \"\\n\" + '| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                        val_loss, math.exp(val_loss)) + '-' * 89\n",
        "    print(log)\n",
        "\n",
        "    # Save the model if the validation loss is the best we've seen so far.\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        lr /= 4.0\n",
        "        counter += 1\n",
        "\n",
        "        if counter == 5:\n",
        "            break\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save_path, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data, \"test\")\n",
        "\n",
        "log = ('=' * 89) + '| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)) + ('=' * 89)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 10001\n",
            "valid: 10001\n",
            "test: 10001\n",
            "dictionary size: 10001\n",
            "RNNModel(\n",
            "  (drop): Dropout(p=0.2, inplace=False)\n",
            "  (encoder): Embedding(10001, 200)\n",
            "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
            "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:134: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 1327 batches | lr 20.00 | ms/batch  8.67 | word_loss  6.93 | ppl  1019.09\n",
            "| epoch   1 |   400/ 1327 batches | lr 20.00 | ms/batch  8.32 | word_loss  6.30 | ppl   545.50\n",
            "| epoch   1 |   600/ 1327 batches | lr 20.00 | ms/batch  8.35 | word_loss  6.04 | ppl   421.60\n",
            "| epoch   1 |   800/ 1327 batches | lr 20.00 | ms/batch  8.31 | word_loss  5.77 | ppl   319.78\n",
            "| epoch   1 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.30 | word_loss  5.63 | ppl   278.45\n",
            "| epoch   1 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.33 | word_loss  5.47 | ppl   237.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 11.36s | valid loss  5.38 | valid ppl   217.10-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 1327 batches | lr 20.00 | ms/batch  8.42 | word_loss  5.38 | ppl   217.88\n",
            "| epoch   2 |   400/ 1327 batches | lr 20.00 | ms/batch  8.45 | word_loss  5.31 | ppl   203.13\n",
            "| epoch   2 |   600/ 1327 batches | lr 20.00 | ms/batch  8.37 | word_loss  5.27 | ppl   194.35\n",
            "| epoch   2 |   800/ 1327 batches | lr 20.00 | ms/batch  8.43 | word_loss  5.16 | ppl   173.94\n",
            "| epoch   2 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.40 | word_loss  5.15 | ppl   172.92\n",
            "| epoch   2 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.46 | word_loss  5.04 | ppl   154.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 11.41s | valid loss  5.09 | valid ppl   161.62-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1327 batches | lr 20.00 | ms/batch  8.52 | word_loss  5.05 | ppl   155.37\n",
            "| epoch   3 |   400/ 1327 batches | lr 20.00 | ms/batch  8.40 | word_loss  5.02 | ppl   151.52\n",
            "| epoch   3 |   600/ 1327 batches | lr 20.00 | ms/batch  8.39 | word_loss  5.00 | ppl   148.32\n",
            "| epoch   3 |   800/ 1327 batches | lr 20.00 | ms/batch  8.30 | word_loss  4.92 | ppl   136.35\n",
            "| epoch   3 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.12 | word_loss  4.95 | ppl   140.69\n",
            "| epoch   3 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.35 | word_loss  4.83 | ppl   125.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 11.34s | valid loss  4.94 | valid ppl   140.32-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1327 batches | lr 20.00 | ms/batch  8.28 | word_loss  4.86 | ppl   129.05\n",
            "| epoch   4 |   400/ 1327 batches | lr 20.00 | ms/batch  8.25 | word_loss  4.85 | ppl   128.17\n",
            "| epoch   4 |   600/ 1327 batches | lr 20.00 | ms/batch  8.46 | word_loss  4.84 | ppl   125.95\n",
            "| epoch   4 |   800/ 1327 batches | lr 20.00 | ms/batch  8.23 | word_loss  4.77 | ppl   117.84\n",
            "| epoch   4 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.33 | word_loss  4.81 | ppl   122.80\n",
            "| epoch   4 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.42 | word_loss  4.70 | ppl   109.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 11.30s | valid loss  4.86 | valid ppl   128.56-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1327 batches | lr 20.00 | ms/batch  8.47 | word_loss  4.73 | ppl   113.85\n",
            "| epoch   5 |   400/ 1327 batches | lr 20.00 | ms/batch  8.39 | word_loss  4.74 | ppl   114.02\n",
            "| epoch   5 |   600/ 1327 batches | lr 20.00 | ms/batch  8.42 | word_loss  4.72 | ppl   112.27\n",
            "| epoch   5 |   800/ 1327 batches | lr 20.00 | ms/batch  8.42 | word_loss  4.66 | ppl   105.64\n",
            "| epoch   5 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.40 | word_loss  4.71 | ppl   111.55\n",
            "| epoch   5 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.32 | word_loss  4.60 | ppl    99.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 11.36s | valid loss  4.81 | valid ppl   122.45-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 1327 batches | lr 20.00 | ms/batch  8.48 | word_loss  4.64 | ppl   104.01\n",
            "| epoch   6 |   400/ 1327 batches | lr 20.00 | ms/batch  8.35 | word_loss  4.65 | ppl   104.19\n",
            "| epoch   6 |   600/ 1327 batches | lr 20.00 | ms/batch  8.23 | word_loss  4.64 | ppl   103.75\n",
            "| epoch   6 |   800/ 1327 batches | lr 20.00 | ms/batch  8.46 | word_loss  4.58 | ppl    97.08\n",
            "| epoch   6 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.41 | word_loss  4.64 | ppl   103.93\n",
            "| epoch   6 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.46 | word_loss  4.52 | ppl    92.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 11.39s | valid loss  4.78 | valid ppl   118.78-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 1327 batches | lr 20.00 | ms/batch  8.54 | word_loss  4.58 | ppl    97.30\n",
            "| epoch   7 |   400/ 1327 batches | lr 20.00 | ms/batch  8.43 | word_loss  4.58 | ppl    97.72\n",
            "| epoch   7 |   600/ 1327 batches | lr 20.00 | ms/batch  8.50 | word_loss  4.57 | ppl    96.92\n",
            "| epoch   7 |   800/ 1327 batches | lr 20.00 | ms/batch  8.51 | word_loss  4.52 | ppl    91.41\n",
            "| epoch   7 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.48 | word_loss  4.58 | ppl    97.92\n",
            "| epoch   7 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.48 | word_loss  4.46 | ppl    86.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 11.51s | valid loss  4.75 | valid ppl   115.68-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 1327 batches | lr 20.00 | ms/batch  8.51 | word_loss  4.53 | ppl    92.30\n",
            "| epoch   8 |   400/ 1327 batches | lr 20.00 | ms/batch  8.50 | word_loss  4.53 | ppl    92.65\n",
            "| epoch   8 |   600/ 1327 batches | lr 20.00 | ms/batch  8.54 | word_loss  4.52 | ppl    91.98\n",
            "| epoch   8 |   800/ 1327 batches | lr 20.00 | ms/batch  8.49 | word_loss  4.46 | ppl    86.83\n",
            "| epoch   8 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.48 | word_loss  4.54 | ppl    93.78\n",
            "| epoch   8 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.55 | word_loss  4.42 | ppl    82.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 11.53s | valid loss  4.73 | valid ppl   113.82-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 1327 batches | lr 20.00 | ms/batch  8.55 | word_loss  4.48 | ppl    87.90\n",
            "| epoch   9 |   400/ 1327 batches | lr 20.00 | ms/batch  8.52 | word_loss  4.48 | ppl    88.22\n",
            "| epoch   9 |   600/ 1327 batches | lr 20.00 | ms/batch  8.52 | word_loss  4.48 | ppl    87.84\n",
            "| epoch   9 |   800/ 1327 batches | lr 20.00 | ms/batch  8.55 | word_loss  4.42 | ppl    83.18\n",
            "| epoch   9 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.51 | word_loss  4.50 | ppl    89.68\n",
            "| epoch   9 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.53 | word_loss  4.38 | ppl    79.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 11.57s | valid loss  4.72 | valid ppl   112.62-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 1327 batches | lr 20.00 | ms/batch  8.66 | word_loss  4.44 | ppl    84.88\n",
            "| epoch  10 |   400/ 1327 batches | lr 20.00 | ms/batch  8.55 | word_loss  4.44 | ppl    84.89\n",
            "| epoch  10 |   600/ 1327 batches | lr 20.00 | ms/batch  8.62 | word_loss  4.44 | ppl    84.88\n",
            "| epoch  10 |   800/ 1327 batches | lr 20.00 | ms/batch  8.30 | word_loss  4.39 | ppl    80.24\n",
            "| epoch  10 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.34 | word_loss  4.46 | ppl    86.72\n",
            "| epoch  10 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.60 | word_loss  4.34 | ppl    76.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 11.55s | valid loss  4.71 | valid ppl   111.18-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 1327 batches | lr 20.00 | ms/batch  8.42 | word_loss  4.41 | ppl    82.09\n",
            "| epoch  11 |   400/ 1327 batches | lr 20.00 | ms/batch  8.60 | word_loss  4.41 | ppl    82.22\n",
            "| epoch  11 |   600/ 1327 batches | lr 20.00 | ms/batch  8.57 | word_loss  4.41 | ppl    82.36\n",
            "| epoch  11 |   800/ 1327 batches | lr 20.00 | ms/batch  8.59 | word_loss  4.35 | ppl    77.81\n",
            "| epoch  11 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.49 | word_loss  4.44 | ppl    84.50\n",
            "| epoch  11 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.40 | word_loss  4.32 | ppl    74.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 11.56s | valid loss  4.71 | valid ppl   110.65-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 1327 batches | lr 20.00 | ms/batch  8.59 | word_loss  4.38 | ppl    79.48\n",
            "| epoch  12 |   400/ 1327 batches | lr 20.00 | ms/batch  8.41 | word_loss  4.39 | ppl    80.56\n",
            "| epoch  12 |   600/ 1327 batches | lr 20.00 | ms/batch  8.58 | word_loss  4.38 | ppl    79.80\n",
            "| epoch  12 |   800/ 1327 batches | lr 20.00 | ms/batch  8.62 | word_loss  4.33 | ppl    75.89\n",
            "| epoch  12 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.52 | word_loss  4.41 | ppl    82.23\n",
            "| epoch  12 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.62 | word_loss  4.29 | ppl    72.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 11.60s | valid loss  4.70 | valid ppl   109.85-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 1327 batches | lr 20.00 | ms/batch  8.65 | word_loss  4.36 | ppl    77.98\n",
            "| epoch  13 |   400/ 1327 batches | lr 20.00 | ms/batch  8.58 | word_loss  4.36 | ppl    78.55\n",
            "| epoch  13 |   600/ 1327 batches | lr 20.00 | ms/batch  8.59 | word_loss  4.36 | ppl    78.07\n",
            "| epoch  13 |   800/ 1327 batches | lr 20.00 | ms/batch  8.57 | word_loss  4.31 | ppl    74.22\n",
            "| epoch  13 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.60 | word_loss  4.39 | ppl    80.26\n",
            "| epoch  13 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.63 | word_loss  4.27 | ppl    71.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 11.66s | valid loss  4.70 | valid ppl   109.41-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 1327 batches | lr 20.00 | ms/batch  8.63 | word_loss  4.33 | ppl    76.11\n",
            "| epoch  14 |   400/ 1327 batches | lr 20.00 | ms/batch  8.56 | word_loss  4.34 | ppl    76.79\n",
            "| epoch  14 |   600/ 1327 batches | lr 20.00 | ms/batch  8.60 | word_loss  4.34 | ppl    76.52\n",
            "| epoch  14 |   800/ 1327 batches | lr 20.00 | ms/batch  8.59 | word_loss  4.28 | ppl    72.50\n",
            "| epoch  14 |  1000/ 1327 batches | lr 20.00 | ms/batch  8.61 | word_loss  4.37 | ppl    78.90\n",
            "| epoch  14 |  1200/ 1327 batches | lr 20.00 | ms/batch  8.58 | word_loss  4.25 | ppl    69.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 11.65s | valid loss  4.70 | valid ppl   109.81-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 1327 batches | lr 5.00 | ms/batch  8.63 | word_loss  4.30 | ppl    73.56\n",
            "| epoch  15 |   400/ 1327 batches | lr 5.00 | ms/batch  8.61 | word_loss  4.26 | ppl    70.58\n",
            "| epoch  15 |   600/ 1327 batches | lr 5.00 | ms/batch  8.59 | word_loss  4.22 | ppl    68.08\n",
            "| epoch  15 |   800/ 1327 batches | lr 5.00 | ms/batch  8.61 | word_loss  4.13 | ppl    62.34\n",
            "| epoch  15 |  1000/ 1327 batches | lr 5.00 | ms/batch  8.57 | word_loss  4.18 | ppl    65.46\n",
            "| epoch  15 |  1200/ 1327 batches | lr 5.00 | ms/batch  8.60 | word_loss  4.03 | ppl    56.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 11.66s | valid loss  4.63 | valid ppl   102.29-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 1327 batches | lr 5.00 | ms/batch  8.62 | word_loss  4.19 | ppl    66.12\n",
            "| epoch  16 |   400/ 1327 batches | lr 5.00 | ms/batch  8.60 | word_loss  4.17 | ppl    64.90\n",
            "| epoch  16 |   600/ 1327 batches | lr 5.00 | ms/batch  8.63 | word_loss  4.16 | ppl    64.04\n",
            "| epoch  16 |   800/ 1327 batches | lr 5.00 | ms/batch  8.58 | word_loss  4.08 | ppl    59.28\n",
            "| epoch  16 |  1000/ 1327 batches | lr 5.00 | ms/batch  8.58 | word_loss  4.14 | ppl    63.06\n",
            "| epoch  16 |  1200/ 1327 batches | lr 5.00 | ms/batch  8.54 | word_loss  4.00 | ppl    54.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 11.65s | valid loss  4.62 | valid ppl   101.15-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 1327 batches | lr 5.00 | ms/batch  8.53 | word_loss  4.15 | ppl    63.26\n",
            "| epoch  17 |   400/ 1327 batches | lr 5.00 | ms/batch  8.29 | word_loss  4.14 | ppl    62.82\n",
            "| epoch  17 |   600/ 1327 batches | lr 5.00 | ms/batch  8.53 | word_loss  4.12 | ppl    61.45\n",
            "| epoch  17 |   800/ 1327 batches | lr 5.00 | ms/batch  8.63 | word_loss  4.05 | ppl    57.29\n",
            "| epoch  17 |  1000/ 1327 batches | lr 5.00 | ms/batch  8.39 | word_loss  4.12 | ppl    61.55\n",
            "| epoch  17 |  1200/ 1327 batches | lr 5.00 | ms/batch  8.60 | word_loss  3.98 | ppl    53.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 11.53s | valid loss  4.61 | valid ppl   100.51-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 1327 batches | lr 5.00 | ms/batch  8.64 | word_loss  4.12 | ppl    61.60\n",
            "| epoch  18 |   400/ 1327 batches | lr 5.00 | ms/batch  8.59 | word_loss  4.11 | ppl    61.11\n",
            "| epoch  18 |   600/ 1327 batches | lr 5.00 | ms/batch  8.58 | word_loss  4.09 | ppl    59.81\n",
            "| epoch  18 |   800/ 1327 batches | lr 5.00 | ms/batch  8.59 | word_loss  4.03 | ppl    56.31\n",
            "| epoch  18 |  1000/ 1327 batches | lr 5.00 | ms/batch  8.36 | word_loss  4.10 | ppl    60.46\n",
            "| epoch  18 |  1200/ 1327 batches | lr 5.00 | ms/batch  8.56 | word_loss  3.97 | ppl    52.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 11.60s | valid loss  4.61 | valid ppl   100.06-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 1327 batches | lr 5.00 | ms/batch  8.69 | word_loss  4.09 | ppl    59.76\n",
            "| epoch  19 |   400/ 1327 batches | lr 5.00 | ms/batch  8.57 | word_loss  4.09 | ppl    59.69\n",
            "| epoch  19 |   600/ 1327 batches | lr 5.00 | ms/batch  8.59 | word_loss  4.08 | ppl    58.97\n",
            "| epoch  19 |   800/ 1327 batches | lr 5.00 | ms/batch  8.59 | word_loss  4.02 | ppl    55.43\n",
            "| epoch  19 |  1000/ 1327 batches | lr 5.00 | ms/batch  8.59 | word_loss  4.09 | ppl    59.60\n",
            "| epoch  19 |  1200/ 1327 batches | lr 5.00 | ms/batch  8.58 | word_loss  3.96 | ppl    52.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 11.65s | valid loss  4.60 | valid ppl    99.87-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 1327 batches | lr 5.00 | ms/batch  8.63 | word_loss  4.08 | ppl    58.94\n",
            "| epoch  20 |   400/ 1327 batches | lr 5.00 | ms/batch  8.61 | word_loss  4.07 | ppl    58.44\n",
            "| epoch  20 |   600/ 1327 batches | lr 5.00 | ms/batch  8.57 | word_loss  4.06 | ppl    58.03\n",
            "| epoch  20 |   800/ 1327 batches | lr 5.00 | ms/batch  8.61 | word_loss  4.00 | ppl    54.51\n",
            "| epoch  20 |  1000/ 1327 batches | lr 5.00 | ms/batch  8.59 | word_loss  4.07 | ppl    58.55\n",
            "| epoch  20 |  1200/ 1327 batches | lr 5.00 | ms/batch  8.65 | word_loss  3.94 | ppl    51.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 11.67s | valid loss  4.60 | valid ppl    99.84-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOrF4_MChve6",
        "colab_type": "text"
      },
      "source": [
        "### Generating a sentence\n",
        "How about if we generate some sentences from the trained model?\n",
        "\n",
        "Let's take our pre-trained model to generate some words. In this part, we take a word `you` to start the generation. We keep the hidden states of the model and the predicted word, and use it to generate the next word. \n",
        "\n",
        "This process is done in an autogressive manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TY3ow-XhKFM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "756b5180-033f-4acf-e637-9db6c8ca47bc"
      },
      "source": [
        "temperature = 1\n",
        "words = 10\n",
        "\n",
        "with open(save_path, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "model.eval()\n",
        "\n",
        "start_word = \"the\"\n",
        "\n",
        "with torch.no_grad():\n",
        "  ntokens = len(corpus.dictionary)\n",
        "  hidden = model.init_hidden(1)\n",
        "  input = Variable(torch.rand(1, 1).mul(ntokens).long(), volatile=True)\n",
        "  input.data.fill_(corpus.dictionary.word2idx[start_word])\n",
        "  if args.cuda:\n",
        "      input.data = input.data.cuda()\n",
        "\n",
        "  sentences = start_word + \" \"\n",
        "  for i in range(words):\n",
        "      output, hidden = model(input, hidden)\n",
        "      word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
        "      word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "      input.data.fill_(word_idx)\n",
        "      word = corpus.dictionary.idx2word[word_idx.item()]\n",
        "      sentences += word + \" \"\n",
        "  print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the <unk> manufacturer of new greed when the trip is a \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf-M4j9mOSgP",
        "colab_type": "text"
      },
      "source": [
        "## Revisiting the Sentiment Analysis\n",
        "Let's implement `LSTMModel` and `TransformerModel` for IMDB sentiment text analysis.\n",
        "\n",
        "Import all required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNhe_daqn2IF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9e179907-d44b-49f1-e8be-d7236d0d642a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"device: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.6.0+cu101\n",
            "device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGcU6wtbn9mc",
        "colab_type": "text"
      },
      "source": [
        "Define the hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3eeW7s6n9JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "args = {\n",
        "  \"hidden_size\": 100,\n",
        "  \"output_size\": 1,\n",
        "  \"lr\": 1e-4,\n",
        "  \"seed\": 1234,\n",
        "  \"max_vocab_size\": 1000,\n",
        "  \"batch_size\": 64,\n",
        "  \"num_epoch\": 10,\n",
        "  \"dropout\": 0.0\n",
        "}\n",
        "args = namedtuple('Struct', args.keys())(*args.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsSziiVLoiEZ",
        "colab_type": "text"
      },
      "source": [
        "Define the metric for evaluation. We follow the same function as in Practical 6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5kzo224ohd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch\n",
        "    \"\"\"\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds)) # threshold 0.5\n",
        "    correct = (rounded_preds == y).float() # convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYKJS9YL9CEr",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bus_ooDgls-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8c3e3567-f4c5-424b-c8a2-a4ef82b6e239"
      },
      "source": [
        "from torchtext import datasets\n",
        "from torchtext import data\n",
        "import random\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True) # add spacy tokenizer\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(args.seed))\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=args.max_vocab_size)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# Data\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = args.batch_size, \n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 10.5MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs9rRxOyMvMu",
        "colab_type": "text"
      },
      "source": [
        "### Build an LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSipCu6ZlvQv",
        "colab_type": "text"
      },
      "source": [
        "Let's define the `LSTMModel` for sequence classification. Please note that we are using `pack_padded_sequence` and `pad_packed_sequence` to allow the model to ignore padding during computations (the batch must be sorted)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDF2sPug8kIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, output_size, dropout=0.0, pad_idx=0, num_layer=2):\n",
        "    super(LSTMModel, self).__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_idx)\n",
        "    self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers=num_layer, bidirectional=True)\n",
        "    self.layer = nn.Linear(hidden_size*2, output_size)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, inputs, inputs_len):\n",
        "    \"\"\"\n",
        "      inputs: LongTensor (seq_len, batch_size)\n",
        "    \"\"\"\n",
        "    embedded_inputs = self.drop(self.emb(inputs)) # (seq_len, batch_size, emb_size)\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded_inputs, inputs_len)    \n",
        "    packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "    output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "    hidden = self.drop(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) # (batch size, hid_dim * num directions)\n",
        "    return self.layer(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Rt03So6REP",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrSCIQ1LNlQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "04e6b53b-29a7-4ff9-fc79-320e41f8aef0"
      },
      "source": [
        "model = LSTMModel(len(TEXT.vocab), args.hidden_size, args.output_size, num_layer=2)\n",
        "train_model(model, train_iterator, valid_iterator, \"best_lstm_model.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 train loss:0.622 acc:0.640 valid loss:0.577 acc:0.703 time:15.180s\n",
            "Save model\n",
            "Epoch: 2 train loss:0.498 acc:0.760 valid loss:0.563 acc:0.710 time:14.933s\n",
            "Save model\n",
            "Epoch: 3 train loss:0.464 acc:0.776 valid loss:0.518 acc:0.751 time:15.036s\n",
            "Save model\n",
            "Epoch: 4 train loss:0.411 acc:0.813 valid loss:0.490 acc:0.787 time:15.099s\n",
            "Save model\n",
            "Epoch: 5 train loss:0.427 acc:0.802 valid loss:0.453 acc:0.796 time:15.037s\n",
            "Save model\n",
            "Epoch: 6 train loss:0.389 acc:0.826 valid loss:0.461 acc:0.789 time:15.254s\n",
            "Epoch: 7 train loss:0.380 acc:0.835 valid loss:0.482 acc:0.782 time:15.180s\n",
            "Epoch: 8 train loss:0.331 acc:0.860 valid loss:0.376 acc:0.835 time:15.185s\n",
            "Save model\n",
            "Epoch: 9 train loss:0.320 acc:0.864 valid loss:0.384 acc:0.833 time:15.169s\n",
            "Epoch: 10 train loss:0.286 acc:0.884 valid loss:0.394 acc:0.825 time:15.222s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXpcQQwK6WTd",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViXvPoQJN_-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd5bd529-ca79-442f-8b80-914b70580668"
      },
      "source": [
        "model.load_state_dict(torch.load('best_lstm_model.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iterator)\n",
        "print(f\"test loss:{test_loss:.3f} acc:{test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss:0.352 acc:0.847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zAMDzLBrI6r",
        "colab_type": "text"
      },
      "source": [
        "### Build a Transfomer model\n",
        "Here, we implement `PositionalEncoding` and `TransformerModel`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKwA9u_OnE3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "      super(PositionalEncoding, self).__init__()\n",
        "      self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "      pe = torch.zeros(max_len, d_model)\n",
        "      position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "      div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "      pe[:, 0::2] = torch.sin(position * div_term)\n",
        "      pe[:, 1::2] = torch.cos(position * div_term)\n",
        "      pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "      self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = x + self.pe[:x.size(0), :]\n",
        "      return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, output_size, dropout=0.0, pad_idx=0, num_layer=2):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_idx)\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=4)\n",
        "    self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "    self.pos_encoder = PositionalEncoding(hidden_size, dropout)\n",
        "    self.layer = nn.Linear(hidden_size, output_size)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.src_mask = None\n",
        "\n",
        "  def _generate_square_subsequent_mask(self, sz):\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "  def forward(self, inputs, inputs_len):\n",
        "    \"\"\"\n",
        "      inputs: LongTensor (seq_len, batch_size)\n",
        "    \"\"\"\n",
        "    inputs = inputs.transpose(1, 0)\n",
        "    inputs = self.drop(self.emb(inputs)) # (batch_size, seq_len, emb_size)\n",
        "\n",
        "    if self.src_mask is None or self.src_mask.size(0) != len(inputs):\n",
        "        device = inputs.device\n",
        "        mask = self._generate_square_subsequent_mask(len(inputs)).to(device)\n",
        "        self.src_mask = mask\n",
        "\n",
        "    src = self.encoder(inputs) * math.sqrt(self.hidden_size)\n",
        "    src = self.pos_encoder(src)\n",
        "    pooled_inputs = F.avg_pool2d(src, (src.shape[1], 1)).squeeze(1)  # (batch_size, emb_size)\n",
        "    \n",
        "    return self.layer(pooled_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXS9SWVn6a2h",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI5gGdu-ofUx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "d8621236-95a8-4cff-851e-5865cd897e1e"
      },
      "source": [
        "model = TransformerModel(len(TEXT.vocab), args.hidden_size, args.output_size, num_layer=2)\n",
        "train_model(model, train_iterator, valid_iterator, \"best_transformer_model.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 train loss:0.914 acc:0.512 valid loss:0.701 acc:0.511 time:12.462s\n",
            "Save model\n",
            "Epoch: 2 train loss:0.606 acc:0.662 valid loss:0.592 acc:0.684 time:12.436s\n",
            "Save model\n",
            "Epoch: 3 train loss:0.489 acc:0.765 valid loss:0.499 acc:0.754 time:12.428s\n",
            "Save model\n",
            "Epoch: 4 train loss:0.451 acc:0.789 valid loss:0.456 acc:0.793 time:12.390s\n",
            "Save model\n",
            "Epoch: 5 train loss:0.438 acc:0.798 valid loss:0.436 acc:0.799 time:12.456s\n",
            "Save model\n",
            "Epoch: 6 train loss:0.417 acc:0.812 valid loss:0.418 acc:0.813 time:12.435s\n",
            "Save model\n",
            "Epoch: 7 train loss:0.406 acc:0.816 valid loss:0.417 acc:0.816 time:12.388s\n",
            "Save model\n",
            "Epoch: 8 train loss:0.389 acc:0.826 valid loss:0.407 acc:0.817 time:12.348s\n",
            "Save model\n",
            "Epoch: 9 train loss:0.390 acc:0.829 valid loss:0.405 acc:0.819 time:12.301s\n",
            "Save model\n",
            "Epoch: 10 train loss:0.378 acc:0.832 valid loss:0.475 acc:0.778 time:12.364s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_grtGge7vNax",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3CggoGrvMqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "136faaf0-6162-4b7f-bc40-4bfb6c43e8e7"
      },
      "source": [
        "model.load_state_dict(torch.load('best_transformer_model.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iterator)\n",
        "print(f\"test loss:{test_loss:.3f} acc:{test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-e0532b08a975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_transformer_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"test loss:{test_loss:.3f} acc:{test_acc:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1045\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RNNModel:\n\tMissing key(s) in state_dict: \"encoder.weight\", \"rnn.weight_ih_l0\", \"rnn.weight_hh_l0\", \"rnn.bias_ih_l0\", \"rnn.bias_hh_l0\", \"rnn.weight_ih_l1\", \"rnn.weight_hh_l1\", \"rnn.bias_ih_l1\", \"rnn.bias_hh_l1\", \"decoder.weight\", \"decoder.bias\". \n\tUnexpected key(s) in state_dict: \"emb.weight\", \"pos_encoder.pe\", \"layer.weight\", \"layer.bias\", \"encoder.layers.0.self_attn.in_proj_weight\", \"encoder.layers.0.self_attn.in_proj_bias\", \"encoder.layers.0.self_attn.out_proj.weight\", \"encoder.layers.0.self_attn.out_proj.bias\", \"encoder.layers.0.linear1.weight\", \"encoder.layers.0.linear1.bias\", \"encoder.layers.0.linear2.weight\", \"encoder.layers.0.linear2.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.1.self_attn.in_proj_weight\", \"encoder.layers.1.self_attn.in_proj_bias\", \"encoder.layers.1.self_attn.out_proj.weight\", \"encoder.layers.1.self_attn.out_proj.bias\", \"encoder.layers.1.linear1.weight\", \"encoder.layers.1.linear1.bias\", \"encoder.layers.1.linear2.weight\", \"encoder.layers.1.linear2.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\". "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5rpez6yMyXw",
        "colab_type": "text"
      },
      "source": [
        "### Build a model with a pre-trained BERT model\n",
        "\n",
        "We can also take a pre-trained contextual language model and fine-tune the model for a downstreamed task, such as sentiment analysis.\n",
        "\n",
        "It will take almost forever in this Colab GPU server. Consider to run it in your private / GCP GPU server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxLKI-uFwC62",
        "colab_type": "text"
      },
      "source": [
        "#### Download Huggingface's `transfomers` package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRLgjEpukDpc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "outputId": "0020b458-cda0-4d5f-eac9-69e425c225ad"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\r\u001b[K     |▍                               | 10kB 7.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 4.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 4.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 4.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 5.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 24.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a33c2aed024ff0df9b6f2ea9f8b7fb09e11eda5d635f4cc6888c6a18149cf394\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCZUaqCD6glA",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt0y2IXjhtHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens\n",
        "\n",
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  include_lengths = True,\n",
        "                  tokenize = tokenize,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VARF6MBVnWpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(args.seed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP8mp3vUkjVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL.build_vocab(train_data)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = args.batch_size, \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN1Nmhj5RT81",
        "colab_type": "text"
      },
      "source": [
        "Implement `BERTTransformerModel`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7HGQux8iiee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "class BERTTransformerModel(nn.Module):\n",
        "    def __init__(self, bert, hidden_size, output_size, dropout=0.0):\n",
        "        super(BERTTransformerModel, self).__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, inputs, inputs_len):\n",
        "        \"\"\"\n",
        "          inputs: LongTensor (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "          embedded_inputs = self.bert(inputs)[0]\n",
        "\n",
        "        pooled_inputs = F.avg_pool2d(embedded_inputs, (embedded_inputs.shape[1], 1)).squeeze(1)  # (batch_size, emb_size)\n",
        "        output = self.out(pooled_inputs)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTTKCjw0db9M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "d5beafb4-5d0f-43a9-977d-4fb53801eb83"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "model = BERTTransformerModel(bert, embedding_dim, args.output_size)\n",
        "train_model(model, train_iterator, valid_iterator, \"best_transformer_model.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 train loss:0.649 acc:0.665 valid loss:0.523 acc:0.796 time:1218.773s\n",
            "Save model\n",
            "Epoch: 2 train loss:0.578 acc:0.760 valid loss:0.455 acc:0.804 time:1226.984s\n",
            "Save model\n",
            "Epoch: 3 train loss:0.538 acc:0.782 valid loss:0.414 acc:0.822 time:1228.352s\n",
            "Save model\n",
            "Epoch: 4 train loss:0.509 acc:0.798 valid loss:0.387 acc:0.834 time:1228.644s\n",
            "Save model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}